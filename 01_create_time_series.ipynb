{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "\n",
    "import pandas as pd\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Step 1: Load and Combine CSV Files\n",
    "\n",
    " **Goal:** Aggregate multiple CSV files of the electricity prices from various countries into a single DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = 'data/spain_france_portugal'  # Directory containing the CSV files\n",
    "dfs = []  # List to store each individual DataFrame\n",
    "\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith('.csv'):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        df = pd.read_csv(file_path)\n",
    "        dfs.append(df)\n",
    "\n",
    "combined_df = pd.concat(dfs, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1fbf3bc5aa96465aba4b6a869bb64a79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Dataset:', index=1, options=('europe', 'australia'), value='australia')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "597a1e66177e409db294dabdc04bf870",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(button_style='success', description='Load Dataset', icon='check', style=ButtonStyle(), tooltip='Click t…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --- Create dropdown ---\n",
    "option_selector = widgets.Dropdown(\n",
    "    options=['europe', 'australia'],\n",
    "    value='europe',  # initial default\n",
    "    description='Dataset:',\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "# --- Create button ---\n",
    "button = widgets.Button(\n",
    "    description=\"Load Dataset\",\n",
    "    button_style='success',  # 'success', 'info', 'warning', 'danger' or ''\n",
    "    tooltip='Click to load selected dataset',\n",
    "    icon='check'  # (optional) FontAwesome icon\n",
    ")\n",
    "\n",
    "# --- Define button click handler ---\n",
    "def on_button_click(b):\n",
    "    global combined_df\n",
    "    clear_output(wait=True)  # Clears previous output to keep it clean\n",
    "    display(option_selector, button)  # Re-display widgets after clear\n",
    "    \n",
    "    selected_option = option_selector.value\n",
    "    \n",
    "    if selected_option == 'europe':\n",
    "        # Example: create dummy combined_df\n",
    "        combined_df = combined_df.copy()\n",
    "        print(\"✅ Europe dataset selected (combined_df copied).\")\n",
    "        print(combined_df.head())\n",
    "    \n",
    "    elif selected_option == 'australia':\n",
    "        file_path = 'parquet_files/australia_data.parquet'\n",
    "        try:\n",
    "            combined_df = pd.read_parquet(file_path)\n",
    "            print(f\"✅ Australia dataset loaded from {file_path}.\")\n",
    "            print(combined_df.head())\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error loading Australia dataset: {e}\")\n",
    "\n",
    "# --- Attach button click handler ---\n",
    "button.on_click(on_button_click)\n",
    "\n",
    "# --- Display UI ---\n",
    "display(option_selector, button)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MTU (CET/CEST)</th>\n",
       "      <th>Area</th>\n",
       "      <th>Sequence</th>\n",
       "      <th>Day-ahead Price (EUR/MWh)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>01/07/2009 00:00:00 - 01/07/2009 00:05:00</td>\n",
       "      <td>nsw</td>\n",
       "      <td>Without sequence</td>\n",
       "      <td>16.941263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>01/07/2009 00:05:00 - 01/07/2009 00:10:00</td>\n",
       "      <td>nsw</td>\n",
       "      <td>Without sequence</td>\n",
       "      <td>17.709524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>01/07/2009 00:10:00 - 01/07/2009 00:15:00</td>\n",
       "      <td>nsw</td>\n",
       "      <td>Without sequence</td>\n",
       "      <td>17.678644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>01/07/2009 00:15:00 - 01/07/2009 00:20:00</td>\n",
       "      <td>nsw</td>\n",
       "      <td>Without sequence</td>\n",
       "      <td>16.736212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>01/07/2009 00:20:00 - 01/07/2009 00:25:00</td>\n",
       "      <td>nsw</td>\n",
       "      <td>Without sequence</td>\n",
       "      <td>15.638840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5082615</th>\n",
       "      <td>28/02/2019 23:35:00 - 28/02/2019 23:40:00</td>\n",
       "      <td>vic</td>\n",
       "      <td>Without sequence</td>\n",
       "      <td>227.739870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5082616</th>\n",
       "      <td>28/02/2019 23:40:00 - 28/02/2019 23:45:00</td>\n",
       "      <td>vic</td>\n",
       "      <td>Without sequence</td>\n",
       "      <td>152.210680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5082617</th>\n",
       "      <td>28/02/2019 23:45:00 - 28/02/2019 23:50:00</td>\n",
       "      <td>vic</td>\n",
       "      <td>Without sequence</td>\n",
       "      <td>99.163567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5082618</th>\n",
       "      <td>28/02/2019 23:50:00 - 28/02/2019 23:55:00</td>\n",
       "      <td>vic</td>\n",
       "      <td>Without sequence</td>\n",
       "      <td>97.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5082619</th>\n",
       "      <td>28/02/2019 23:55:00 - 01/03/2019 00:00:00</td>\n",
       "      <td>vic</td>\n",
       "      <td>Without sequence</td>\n",
       "      <td>74.226509</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5082620 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    MTU (CET/CEST) Area          Sequence  \\\n",
       "0        01/07/2009 00:00:00 - 01/07/2009 00:05:00  nsw  Without sequence   \n",
       "1        01/07/2009 00:05:00 - 01/07/2009 00:10:00  nsw  Without sequence   \n",
       "2        01/07/2009 00:10:00 - 01/07/2009 00:15:00  nsw  Without sequence   \n",
       "3        01/07/2009 00:15:00 - 01/07/2009 00:20:00  nsw  Without sequence   \n",
       "4        01/07/2009 00:20:00 - 01/07/2009 00:25:00  nsw  Without sequence   \n",
       "...                                            ...  ...               ...   \n",
       "5082615  28/02/2019 23:35:00 - 28/02/2019 23:40:00  vic  Without sequence   \n",
       "5082616  28/02/2019 23:40:00 - 28/02/2019 23:45:00  vic  Without sequence   \n",
       "5082617  28/02/2019 23:45:00 - 28/02/2019 23:50:00  vic  Without sequence   \n",
       "5082618  28/02/2019 23:50:00 - 28/02/2019 23:55:00  vic  Without sequence   \n",
       "5082619  28/02/2019 23:55:00 - 01/03/2019 00:00:00  vic  Without sequence   \n",
       "\n",
       "         Day-ahead Price (EUR/MWh)  \n",
       "0                        16.941263  \n",
       "1                        17.709524  \n",
       "2                        17.678644  \n",
       "3                        16.736212  \n",
       "4                        15.638840  \n",
       "...                            ...  \n",
       "5082615                 227.739870  \n",
       "5082616                 152.210680  \n",
       "5082617                  99.163567  \n",
       "5082618                  97.500000  \n",
       "5082619                  74.226509  \n",
       "\n",
       "[5082620 rows x 4 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Step 2: Examine Columns and Rows in Combined Data\n",
    "\n",
    " **Goal:** Understand the structure of the merged dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The columns are:  ['MTU (CET/CEST)', 'Area', 'Sequence', 'Day-ahead Price (EUR/MWh)']\n",
      "Number of rows:  5082620\n"
     ]
    }
   ],
   "source": [
    "print('The columns are: ', [col for col in combined_df.columns])\n",
    "print('Number of rows: ', len(combined_df))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Step 3: Extract and Clean Start Times\n",
    "\n",
    " **Goal:** Parse the 'MTU (CET/CEST)' column to derive clean start datetime values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [17], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m start_times \u001b[38;5;241m=\u001b[39m combined_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMTU (CET/CEST)\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mstr\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m - \u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mstr[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m      2\u001b[0m start_times_clean \u001b[38;5;241m=\u001b[39m start_times\u001b[38;5;241m.\u001b[39mstr\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m(CET\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m)| \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m(CEST\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m, regex\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m----> 3\u001b[0m combined_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mStart DateTime\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_datetime\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstart_times_clean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdayfirst\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m combined_df\u001b[38;5;241m.\u001b[39mhead()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\core\\tools\\datetimes.py:1112\u001b[0m, in \u001b[0;36mto_datetime\u001b[1;34m(arg, errors, dayfirst, yearfirst, utc, format, exact, unit, infer_datetime_format, origin, cache)\u001b[0m\n\u001b[0;32m   1110\u001b[0m         result \u001b[38;5;241m=\u001b[39m arg\u001b[38;5;241m.\u001b[39mmap(cache_array)\n\u001b[0;32m   1111\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1112\u001b[0m         values \u001b[38;5;241m=\u001b[39m \u001b[43mconvert_listlike\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1113\u001b[0m         result \u001b[38;5;241m=\u001b[39m arg\u001b[38;5;241m.\u001b[39m_constructor(values, index\u001b[38;5;241m=\u001b[39marg\u001b[38;5;241m.\u001b[39mindex, name\u001b[38;5;241m=\u001b[39marg\u001b[38;5;241m.\u001b[39mname)\n\u001b[0;32m   1114\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arg, (ABCDataFrame, abc\u001b[38;5;241m.\u001b[39mMutableMapping)):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\core\\tools\\datetimes.py:488\u001b[0m, in \u001b[0;36m_convert_listlike_datetimes\u001b[1;34m(arg, format, name, utc, unit, errors, dayfirst, yearfirst, exact)\u001b[0m\n\u001b[0;32m    486\u001b[0m \u001b[38;5;66;03m# `format` could be inferred, or user didn't ask for mixed-format parsing.\u001b[39;00m\n\u001b[0;32m    487\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmixed\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 488\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_array_strptime_with_fallback\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mutc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexact\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    490\u001b[0m result, tz_parsed \u001b[38;5;241m=\u001b[39m objects_to_datetime64ns(\n\u001b[0;32m    491\u001b[0m     arg,\n\u001b[0;32m    492\u001b[0m     dayfirst\u001b[38;5;241m=\u001b[39mdayfirst,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    496\u001b[0m     allow_object\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    497\u001b[0m )\n\u001b[0;32m    499\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tz_parsed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    500\u001b[0m     \u001b[38;5;66;03m# We can take a shortcut since the datetime64 numpy array\u001b[39;00m\n\u001b[0;32m    501\u001b[0m     \u001b[38;5;66;03m# is in UTC\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\core\\tools\\datetimes.py:520\u001b[0m, in \u001b[0;36m_array_strptime_with_fallback\u001b[1;34m(arg, name, utc, fmt, exact, errors)\u001b[0m\n\u001b[0;32m    516\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    517\u001b[0m \u001b[38;5;124;03mCall array_strptime, with fallback behavior depending on 'errors'.\u001b[39;00m\n\u001b[0;32m    518\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    519\u001b[0m result, timezones \u001b[38;5;241m=\u001b[39m array_strptime(arg, fmt, exact\u001b[38;5;241m=\u001b[39mexact, errors\u001b[38;5;241m=\u001b[39merrors, utc\u001b[38;5;241m=\u001b[39mutc)\n\u001b[1;32m--> 520\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(tz \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m tz \u001b[38;5;129;01min\u001b[39;00m timezones):\n\u001b[0;32m    521\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _return_parsed_timezone_results(result, timezones, utc, name)\n\u001b[0;32m    523\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _box_as_indexlike(result, utc\u001b[38;5;241m=\u001b[39mutc, name\u001b[38;5;241m=\u001b[39mname)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "start_times = combined_df['MTU (CET/CEST)'].str.split(' - ').str[0]\n",
    "start_times_clean = start_times.str.replace(r' \\(CET\\)| \\(CEST\\)', '', regex=True)\n",
    "combined_df['Start DateTime'] = pd.to_datetime(start_times_clean, dayfirst=True)\n",
    "combined_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Step 4: Filter Data by Time Interval\n",
    "\n",
    " **Goal:** Keep only rows that fall within the desired datetime range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3359   2021-05-21 00:00:00\n",
      "3360   2021-05-21 01:00:00\n",
      "3361   2021-05-21 02:00:00\n",
      "3362   2021-05-21 03:00:00\n",
      "3363   2021-05-21 04:00:00\n",
      "Name: Start DateTime, dtype: datetime64[ns]\n",
      "114928   2025-04-30 19:00:00\n",
      "114929   2025-04-30 20:00:00\n",
      "114930   2025-04-30 21:00:00\n",
      "114931   2025-04-30 22:00:00\n",
      "114932   2025-04-30 23:00:00\n",
      "Name: Start DateTime, dtype: datetime64[ns]\n"
     ]
    }
   ],
   "source": [
    "start_date = pd.to_datetime('2021-05-21')  # Lower bound for filtering\n",
    "end_date = pd.to_datetime('2025-05-01')    # Upper bound (exclusive)\n",
    "filtered_df = combined_df[(combined_df['Start DateTime'] >= start_date) & (combined_df['Start DateTime'] < end_date)]\n",
    "print(filtered_df['Start DateTime'].head(5))\n",
    "print(filtered_df['Start DateTime'].tail(5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Step 5: Detect Irregular Time Intervals\n",
    "\n",
    "Identify gaps or anomalies in the hourly data sequence grouped by Area and Sequence.  These datetimes are like this because of the Daylight Saving Time (DST)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Area          Sequence      Start DateTime        TimeDiff\n",
      "3914   BZN|ES  Without sequence 2021-10-31 02:00:00 0 days 01:00:00\n",
      "3915   BZN|ES  Without sequence 2021-10-31 02:00:00 0 days 00:00:00\n",
      "7442   BZN|ES  Without sequence 2022-03-27 01:00:00 0 days 01:00:00\n",
      "7443   BZN|ES  Without sequence 2022-03-27 03:00:00 0 days 02:00:00\n",
      "12650  BZN|ES  Without sequence 2022-10-30 02:00:00 0 days 01:00:00\n",
      "12651  BZN|ES  Without sequence 2022-10-30 02:00:00 0 days 00:00:00\n",
      "16178  BZN|ES  Without sequence 2023-03-26 01:00:00 0 days 01:00:00\n",
      "16179  BZN|ES  Without sequence 2023-03-26 03:00:00 0 days 02:00:00\n",
      "21386  BZN|ES  Without sequence 2023-10-29 02:00:00 0 days 01:00:00\n",
      "21387  BZN|ES  Without sequence 2023-10-29 02:00:00 0 days 00:00:00\n"
     ]
    }
   ],
   "source": [
    "filtered_df = filtered_df.sort_values(['Area', 'Sequence', 'Start DateTime']).reset_index(drop=True)\n",
    "filtered_df['TimeDiff'] = filtered_df.groupby(['Area', 'Sequence'])['Start DateTime'].diff()\n",
    "filtered_df['NonHourly'] = (filtered_df['TimeDiff'] != pd.Timedelta(hours=1)) & (~filtered_df['TimeDiff'].isna())\n",
    "\n",
    "problem_indices = filtered_df.index[filtered_df['NonHourly']]\n",
    "context_indices = problem_indices.union(problem_indices - 1)\n",
    "\n",
    "irregular_context = filtered_df.loc[context_indices].sort_values(['Area', 'Sequence', 'Start DateTime'])\n",
    "print(irregular_context[['Area', 'Sequence', 'Start DateTime', 'TimeDiff']].head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Step 6: Export Filtered Data\n",
    "\n",
    "Save the cleaned and validated dataset to a Parquet file for further use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df.to_parquet('parquet_files/filtered_data.parquet', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
