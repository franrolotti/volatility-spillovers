{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Attention-Based Spillover Table Using Feature-Level Attention\n",
    "\n",
    "\n",
    "\n",
    " This version creates a custom attention mechanism across features (vech dimensions)\n",
    "\n",
    " and extracts directional spillover strength from learned attention weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load PIT-transformed vech data\n",
    "option = 'europe'\n",
    "pit_vech = pd.read_parquet(f\"parquet_files/pit_transformed_vech_{option}.parquet\")\n",
    "pit_vech = pit_vech.dropna()\n",
    "columns = pit_vech.columns\n",
    "scaler = MinMaxScaler()\n",
    "data_scaled = scaler.fit_transform(pit_vech)\n",
    "\n",
    "# Sequence creation\n",
    "def create_sequences(data, window):\n",
    "    X, Y = [], []\n",
    "    for i in range(len(data) - window):\n",
    "        X.append(data[i:i+window])\n",
    "        Y.append(data[i+window])\n",
    "    return np.array(X), np.array(Y)\n",
    "\n",
    "window = 10\n",
    "X, Y = create_sequences(data_scaled, window)\n",
    "X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "Y_tensor = torch.tensor(Y, dtype=torch.float32)\n",
    "dataset = TensorDataset(X_tensor, Y_tensor)\n",
    "dataloader = DataLoader(dataset, batch_size=64, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Attention Layer Across Features\n",
    "class FeatureAttentionModel(nn.Module):\n",
    "    def __init__(self, num_features):\n",
    "        super().__init__()\n",
    "        self.attn = nn.Linear(num_features, num_features, bias=False)\n",
    "        self.out = nn.Linear(num_features, num_features)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: [batch, time, features]\n",
    "        x_last = x[:, -1, :]  # use last time step\n",
    "        weights = torch.softmax(self.attn.weight, dim=1)  # feature-to-feature attention\n",
    "        attended = torch.matmul(x_last, weights.T)\n",
    "        out = self.out(attended)\n",
    "        return out, weights.detach()\n",
    "\n",
    "model = FeatureAttentionModel(num_features=X.shape[2])\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Step 2: Train Attention Model and Collect Attention Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "all_attention_matrices = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for xb, yb in dataloader:\n",
    "        pred, weights = model(xb)\n",
    "        loss = criterion(pred, yb)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Capture attention matrices (one per batch)\n",
    "        all_attention_matrices.append(weights.detach().cpu().numpy())\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs} - Loss: {loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Step 3: Compute Average Attention Matrix (Feature-to-Feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stack and average attention weights\n",
    "attn_array = np.stack(all_attention_matrices)  # shape: [num_batches, features, features]\n",
    "attn_avg = attn_array.mean(axis=0)  # shape: [features, features]\n",
    "\n",
    "# Normalize rows to sum to 1 (spillover matrix)\n",
    "spillover_matrix = attn_avg / attn_avg.sum(axis=1, keepdims=True) * 100\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Step 4: Construct Spillover Table (TO, FROM, NET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directional_to = spillover_matrix.sum(axis=1) - np.diag(spillover_matrix)\n",
    "directional_from = spillover_matrix.sum(axis=0) - np.diag(spillover_matrix)\n",
    "net_directional = directional_to - directional_from\n",
    "\n",
    "spillover_table = pd.DataFrame(spillover_matrix,\n",
    "                               index=columns,\n",
    "                               columns=columns)\n",
    "spillover_table[\"Directional FROM others\"] = directional_from\n",
    "spillover_table.loc[\"Directional TO others\"] = list(directional_to) + [directional_to.sum()]\n",
    "spillover_table.loc[\"NET Directional\"] = list(net_directional) + [np.nan]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Step 5: Display and Save Spillover Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "display(spillover_table.round(2))\n",
    "\n",
    "caption = f\"\"\"\n",
    "**Table X**: Attention-Based Spillover Table (Feature-Level Weights)\n",
    "\n",
    "This table is derived from a neural attention model trained to forecast vectorized PIT-transformed realized variances and covariances. \n",
    "The attention matrix is interpreted as a nonlinear spillover mapping between variables. Rows are normalized to sum to 100%.\n",
    "\n",
    "**Directional spillovers (TO / FROM / NET)** reflect learned influences based on attention weights aggregated over all training samples.\n",
    "\"\"\"\n",
    "display(Markdown(caption))\n",
    "\n",
    "# Save table\n",
    "spillover_table.to_parquet(f\"parquet_files/attention_spillover_table_{option}.parquet\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
